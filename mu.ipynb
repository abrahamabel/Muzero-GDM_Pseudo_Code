{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install future\n",
    "%pip install numpy\n",
    "%pip install tensorflow\n",
    "%pip install muzero\n",
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pseudocode description of the MuZero algorithm.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lint as: python3\n",
    "\"\"\"Pseudocode description of the MuZero algorithm.\"\"\"\n",
    "# pylint: disable=unused-argument\n",
    "# pylint: disable=missing-docstring\n",
    "# pylint: disable=g-explicit-length-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division,\n",
    "                        print_function, unicode_literals)\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import typing\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\abrah\\.conda\\envs\\dl_tutorial\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Helpers ##########\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float('inf')\n",
    "\n",
    "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "  \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
    "\n",
    "  def __init__(self, known_bounds: Optional[KnownBounds]):\n",
    "    self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "    self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "  def update(self, value: float):\n",
    "    self.maximum = max(self.maximum, value)\n",
    "    self.minimum = min(self.minimum, value)\n",
    "\n",
    "  def normalize(self, value: float) -> float:\n",
    "    if self.maximum > self.minimum:\n",
    "      # We normalize only when we have set the maximum and minimum values.\n",
    "      return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "    return value\n",
    "\n",
    "\n",
    "class MuZeroConfig(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               action_space_size: int,\n",
    "               max_moves: int,\n",
    "               discount: float,\n",
    "               dirichlet_alpha: float,\n",
    "               num_simulations: int,\n",
    "               batch_size: int,\n",
    "               td_steps: int,\n",
    "               num_actors: int,\n",
    "               lr_init: float,\n",
    "               lr_decay_steps: float,\n",
    "               visit_softmax_temperature_fn,\n",
    "               known_bounds: Optional[KnownBounds] = None):\n",
    "    ### Self-Play\n",
    "    self.action_space_size = action_space_size\n",
    "    self.num_actors = num_actors\n",
    "\n",
    "    self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
    "    self.max_moves = max_moves\n",
    "    self.num_simulations = num_simulations\n",
    "    self.discount = discount\n",
    "\n",
    "    # Root prior exploration noise.\n",
    "    self.root_dirichlet_alpha = dirichlet_alpha\n",
    "    self.root_exploration_fraction = 0.25\n",
    "\n",
    "    # UCB formula\n",
    "    self.pb_c_base = 19652\n",
    "    self.pb_c_init = 1.25\n",
    "\n",
    "    # If we already have some information about which values occur in the\n",
    "    # environment, we can use them to initialize the rescaling.\n",
    "    # This is not strictly necessary, but establishes identical behaviour to\n",
    "    # AlphaZero in board games.\n",
    "    self.known_bounds = known_bounds\n",
    "\n",
    "    ### Training\n",
    "    self.training_steps = int(1000e3)\n",
    "    self.checkpoint_interval = int(1e3)\n",
    "    self.window_size = int(1e6)\n",
    "    self.batch_size = batch_size\n",
    "    self.num_unroll_steps = 5\n",
    "    self.td_steps = td_steps\n",
    "\n",
    "    self.weight_decay = 1e-4\n",
    "    self.momentum = 0.9\n",
    "\n",
    "    # Exponential learning rate schedule\n",
    "    self.lr_init = lr_init\n",
    "    self.lr_decay_rate = 0.1\n",
    "    self.lr_decay_steps = lr_decay_steps\n",
    "\n",
    "  def new_game(self):\n",
    "    return Game(self.action_space_size, self.discount)\n",
    "\n",
    "\n",
    "def make_board_game_config(action_space_size: int, max_moves: int,\n",
    "                           dirichlet_alpha: float,\n",
    "                           lr_init: float) -> MuZeroConfig:\n",
    "\n",
    "  def visit_softmax_temperature(num_moves, training_steps):\n",
    "    if num_moves < 30:\n",
    "      return 1.0\n",
    "    else:\n",
    "      return 0.0  # Play according to the max.\n",
    "\n",
    "  return MuZeroConfig(\n",
    "      action_space_size=action_space_size,\n",
    "      max_moves=max_moves,\n",
    "      discount=1.0,\n",
    "      dirichlet_alpha=dirichlet_alpha,\n",
    "      num_simulations=800,\n",
    "      batch_size=2048,\n",
    "      td_steps=max_moves,  # Always use Monte Carlo return.\n",
    "      num_actors=3000,\n",
    "      lr_init=lr_init,\n",
    "      lr_decay_steps=400e3,\n",
    "      visit_softmax_temperature_fn=visit_softmax_temperature,\n",
    "      known_bounds=KnownBounds(-1, 1))\n",
    "\n",
    "\n",
    "def make_go_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=362, max_moves=722, dirichlet_alpha=0.03, lr_init=0.01)\n",
    "\n",
    "\n",
    "def make_chess_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=4672, max_moves=512, dirichlet_alpha=0.3, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_shogi_config() -> MuZeroConfig:\n",
    "  return make_board_game_config(\n",
    "      action_space_size=11259, max_moves=512, dirichlet_alpha=0.15, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_atari_config() -> MuZeroConfig:\n",
    "\n",
    "  def visit_softmax_temperature(num_moves, training_steps):\n",
    "    if training_steps < 500e3:\n",
    "      return 1.0\n",
    "    elif training_steps < 750e3:\n",
    "      return 0.5\n",
    "    else:\n",
    "      return 0.25\n",
    "\n",
    "  return MuZeroConfig(\n",
    "      action_space_size=18,\n",
    "      max_moves=27000,  # Half an hour at action repeat 4.\n",
    "      discount=0.997,\n",
    "      dirichlet_alpha=0.25,\n",
    "      num_simulations=50,\n",
    "      batch_size=1024,\n",
    "      td_steps=10,\n",
    "      num_actors=350,\n",
    "      lr_init=0.05,\n",
    "      lr_decay_steps=350e3,\n",
    "      visit_softmax_temperature_fn=visit_softmax_temperature)\n",
    "\n",
    "\n",
    "class Action(object):\n",
    "\n",
    "  def __init__(self, index: int):\n",
    "    self.index = index\n",
    "\n",
    "  def __hash__(self):\n",
    "    return self.index\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    return self.index == other.index\n",
    "\n",
    "  def __gt__(self, other):\n",
    "    return self.index > other.index\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "  pass\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "  def __init__(self, prior: float):\n",
    "    self.visit_count = 0\n",
    "    self.to_play = -1\n",
    "    self.prior = prior\n",
    "    self.value_sum = 0\n",
    "    self.children = {}\n",
    "    self.hidden_state = None\n",
    "    self.reward = 0\n",
    "\n",
    "  def expanded(self) -> bool:\n",
    "    return len(self.children) > 0\n",
    "\n",
    "  def value(self) -> float:\n",
    "    if self.visit_count == 0:\n",
    "      return 0\n",
    "    return self.value_sum / self.visit_count\n",
    "\n",
    "\n",
    "class ActionHistory(object):\n",
    "  \"\"\"Simple history container used inside the search.\n",
    "\n",
    "  Only used to keep track of the actions executed.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, history: List[Action], action_space_size: int):\n",
    "    self.history = list(history)\n",
    "    self.action_space_size = action_space_size\n",
    "\n",
    "  def clone(self):\n",
    "    return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "  def add_action(self, action: Action):\n",
    "    self.history.append(action)\n",
    "\n",
    "  def last_action(self) -> Action:\n",
    "    return self.history[-1]\n",
    "\n",
    "  def action_space(self) -> List[Action]:\n",
    "    return [Action(i) for i in range(self.action_space_size)]\n",
    "\n",
    "  def to_play(self) -> Player:\n",
    "    return Player()\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "  \"\"\"The environment MuZero is interacting with.\"\"\"\n",
    "\n",
    "  def step(self, action):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Game(object):\n",
    "  \"\"\"A single episode of interaction with the environment.\"\"\"\n",
    "\n",
    "  def __init__(self, action_space_size: int, discount: float):\n",
    "    self.environment = Environment()  # Game specific environment.\n",
    "    self.history = []\n",
    "    self.rewards = []\n",
    "    self.child_visits = []\n",
    "    self.root_values = []\n",
    "    self.action_space_size = action_space_size\n",
    "    self.discount = discount\n",
    "\n",
    "  def terminal(self) -> bool:\n",
    "    # Game specific termination rules.\n",
    "    pass\n",
    "\n",
    "  def legal_actions(self) -> List[Action]:\n",
    "    # Game specific calculation of legal actions.\n",
    "    return []\n",
    "\n",
    "  def apply(self, action: Action):\n",
    "    reward = self.environment.step(action)\n",
    "    self.rewards.append(reward)\n",
    "    self.history.append(action)\n",
    "\n",
    "  def store_search_statistics(self, root: Node):\n",
    "    sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "    action_space = (Action(index) for index in range(self.action_space_size))\n",
    "    self.child_visits.append([\n",
    "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
    "        for a in action_space\n",
    "    ])\n",
    "    self.root_values.append(root.value())\n",
    "\n",
    "  def make_image(self, state_index: int):\n",
    "    # Game specific feature planes.\n",
    "    return []\n",
    "\n",
    "  def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,\n",
    "                  to_play: Player):\n",
    "    # The value target is the discounted root value of the search tree N steps\n",
    "    # into the future, plus the discounted sum of all rewards until then.\n",
    "    targets = []\n",
    "    for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
    "      bootstrap_index = current_index + td_steps\n",
    "      if bootstrap_index < len(self.root_values):\n",
    "        value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
    "      else:\n",
    "        value = 0\n",
    "\n",
    "      for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
    "        value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
    "\n",
    "      if current_index < len(self.root_values):\n",
    "        targets.append((value, self.rewards[current_index],\n",
    "                        self.child_visits[current_index]))\n",
    "      else:\n",
    "        # States past the end of games are treated as absorbing states.\n",
    "        targets.append((0, 0, []))\n",
    "    return targets\n",
    "\n",
    "  def to_play(self) -> Player:\n",
    "    return Player()\n",
    "\n",
    "  def action_history(self) -> ActionHistory:\n",
    "    return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, config: MuZeroConfig):\n",
    "    self.window_size = config.window_size\n",
    "    self.batch_size = config.batch_size\n",
    "    self.buffer = []\n",
    "\n",
    "  def save_game(self, game):\n",
    "    if len(self.buffer) > self.window_size:\n",
    "      self.buffer.pop(0)\n",
    "    self.buffer.append(game)\n",
    "\n",
    "  def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
    "    games = [self.sample_game() for _ in range(self.batch_size)]\n",
    "    game_pos = [(g, self.sample_position(g)) for g in games]\n",
    "    return [(g.make_image(i), g.history[i:i + num_unroll_steps],\n",
    "             g.make_target(i, num_unroll_steps, td_steps, g.to_play()))\n",
    "            for (g, i) in game_pos]\n",
    "\n",
    "  def sample_game(self) -> Game:\n",
    "    # Sample game from buffer either uniformly or according to some priority.\n",
    "    return self.buffer[0]\n",
    "\n",
    "  def sample_position(self, game) -> int:\n",
    "    # Sample position from game either uniformly or according to some priority.\n",
    "    return -1\n",
    "\n",
    "\n",
    "class NetworkOutput(typing.NamedTuple):\n",
    "  value: float\n",
    "  reward: float\n",
    "  policy_logits: Dict[Action, float]\n",
    "  hidden_state: List[float]\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "  def initial_inference(self, image) -> NetworkOutput:\n",
    "    # representation + prediction function\n",
    "    return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "  def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "    # dynamics + prediction function\n",
    "    return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "  def get_weights(self):\n",
    "    # Returns the weights of this network.\n",
    "    return []\n",
    "\n",
    "  def training_steps(self) -> int:\n",
    "    # How many steps / batches the network has been trained for.\n",
    "    return 0\n",
    "\n",
    "\n",
    "class SharedStorage(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._networks = {}\n",
    "\n",
    "  def latest_network(self) -> Network:\n",
    "    if self._networks:\n",
    "      return self._networks[max(self._networks.keys())]\n",
    "    else:\n",
    "      # policy -> uniform, value -> 0, reward -> 0\n",
    "      return make_uniform_network()\n",
    "\n",
    "  def save_network(self, step: int, network: Network):\n",
    "    self._networks[step] = network\n",
    "\n",
    "\n",
    "##### End Helpers ########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "    # MuZero training is split into two independent parts: Network training and\n",
    "    # self-play data generation.\n",
    "    # These two parts only communicate by transferring the latest network checkpoint\n",
    "    # from the training to the self-play, and the finished games from the self-play\n",
    "    # to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muzero(config: MuZeroConfig):\n",
    "    storage = SharedStorage()\n",
    "    replay_buffer = ReplayBuffer(config)\n",
    "\n",
    "    for _ in range(config.num_actors):\n",
    "        launch_job(run_selfplay, config, storage, replay_buffer)\n",
    "\n",
    "    train_network(config, storage, replay_buffer)\n",
    "\n",
    "    return storage.latest_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entrypoint function muzero is passed a MuZeroConfig object, which stores important information about the parameterisation of the run, such as the action_space_size (number of possible actions) and num_actors (the number of parallel game simulations to spin up). We’ll go through these parameters in more detail as we encounter them in other functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
